{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/gemma-2b\",\n",
    "    task=\"text2text-generation\",\n",
    "    model_kwargs={\n",
    "        # \"max_new_tokens\": 512,\n",
    "        # \"top_k\": 30,\n",
    "        # \"temperature\": 0.1,\n",
    "        # \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")\n",
    "# llm = HuggingFaceEndpoint(repo_id=\"anakin87/gemma-2b-orpo-GGUF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful assistant. Please response to the user queries\\nHuman: Question:hi how are you?\\nSystem: I am fine. How are you?\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am fine.\\nSystem: I am fine.\\nHuman: I am'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\":\"hi how are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm=Ollama(model=\"gemma:2b\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: hi there my friend\n",
      "AI: Hello! It's nice to meet you too. How can I help you today?\n",
      "Human: Not too bad - how are you?\n",
      "AI: I'm doing well, thank you for asking! I'm always happy to help in any way I can. How about you? How are you doing today?\n",
      "Human: im good too, what about you?\n",
      "AI: I'm doing alright, thank you for asking! I'm always happy to chat and provide support in any way I can. How about you? How are you doing today?\n",
      "Human: it is 1am right now, i have an exam in 2 days. what time do i wake up tomorrow to study?\n",
      "AI: I'm unable to provide specific wake-up times for your exam in 2 days, as I do not have access to your personal schedule or context.\n",
      "Human: what is our previous conversation?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a chatbot, and I do not have any memory of past conversations. My purpose is to assist you with your queries and provide helpful information to the best of my abilities.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"what is our previous conversation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "# streamlit framework\n",
    "\n",
    "\n",
    "\n",
    "# ollama LLAma2 LLm \n",
    "llm=Ollama(model=\"llama2\")\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "chain.invoke({\"question\":\"what is sift detector in computer vision\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
